{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a6ecf-ed8a-4333-889b-6e9a59d3b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ.get(\"JAVA_HOME\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e5eaf6-f916-4074-8eeb-0fc8f0af8dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated JAVA_HOME: /usr/lib/jvm/java-21-openjdk-amd64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
    "print(\"Updated JAVA_HOME:\", os.environ[\"JAVA_HOME\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaa17933-f5f7-459d-9571-fefba640e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [(\"John\", 29), (\"Alice\", 25), (\"Bob\", 31)]\n",
    "\n",
    "# Define schema\n",
    "# columns = [\"Name\", \"Age\"]\n",
    "\n",
    "# Create DataFrame\n",
    "# df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show DataFrame\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf8182c-9c10-4db7-a47c-1e958b0b5655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2.5.2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-19244db0-7aa0-4379-ac30-efac51d25bcb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.9.0 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.7 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.16 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.1 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.12.0 in central\n",
      "\tfound org.postgresql#postgresql;42.5.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      ":: resolution report :: resolve 1267ms :: artifacts dl 48ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.12.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.1 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.9.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.5.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.16 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-19244db0-7aa0-4379-ac30-efac51d25bcb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/17ms)\n",
      "25/06/09 18:13:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Spark session initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlightDataPipeline\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0,\"\n",
    "                                    \"org.postgresql:postgresql:42.5.0\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "\n",
    "print(\"ðŸ”¥ Spark session initialized successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb452161-654d-4307-9ab2-183b154458ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"airline\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4e1491f-2392-4444-8146-46c6a9f67436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "998990a4-c9e9-47cc-a1bc-5b0dd9c56d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2acc6863-e910-4052-8bb5-ee4e82cbb09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr,col,from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2612bc4a-4849-471a-8a1d-8ae77e75a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df = df.selectExpr(\"CAST(value AS STRING) as json_value\")\n",
    "# parsed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2983259e-00ec-4e65-8cef-6810cab78114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, FloatType\n",
    "\n",
    "airline_schema = StructType([\n",
    "    StructField(\"flight_number\", StringType(), True),\n",
    "    StructField(\"airline\", StringType(), True),\n",
    "    StructField(\"route_type\", StringType(), True),\n",
    "    StructField(\"expected_departure\", StringType(), True),\n",
    "    StructField(\"actual_departure\", StringType(), True),\n",
    "    StructField(\"capacity\", IntegerType(), True),\n",
    "    StructField(\"passengers\", IntegerType(), True),\n",
    "    StructField(\"ticket_price_range\", StringType(), True),\n",
    "    StructField(\"departure_airport\", StringType(), True),\n",
    "    StructField(\"arrival_airport\", StringType(), True),\n",
    "    StructField(\"fuel_consumption\", FloatType(), True),\n",
    "    StructField(\"carbon_emission\", FloatType(), True),\n",
    "    StructField(\"flight_status\", StringType(), True),\n",
    "    StructField(\"luggage_weight\", FloatType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c56c803-c1f0-4875-8d34-765910c8aa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df=parsed_df.select(from_json(col(\"json_value\"), airline_schema).alias(\"data\")) \n",
    "# json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8919beae-b03d-4a39-94a0-435404808ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- flight_number: string (nullable = true)\n",
      " |    |-- airline: string (nullable = true)\n",
      " |    |-- route_type: string (nullable = true)\n",
      " |    |-- expected_departure: string (nullable = true)\n",
      " |    |-- actual_departure: string (nullable = true)\n",
      " |    |-- capacity: integer (nullable = true)\n",
      " |    |-- passengers: integer (nullable = true)\n",
      " |    |-- ticket_price_range: string (nullable = true)\n",
      " |    |-- departure_airport: string (nullable = true)\n",
      " |    |-- arrival_airport: string (nullable = true)\n",
      " |    |-- fuel_consumption: float (nullable = true)\n",
      " |    |-- carbon_emission: float (nullable = true)\n",
      " |    |-- flight_status: string (nullable = true)\n",
      " |    |-- luggage_weight: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbc60b81-57b8-463f-8a9f-f74050849f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Flatten the parsed DataFrame\n",
    "flat_df = json_df.select(\n",
    "    col(\"data.flight_number\"),\n",
    "    col(\"data.airline\"),\n",
    "    col(\"data.route_type\"),\n",
    "    col(\"data.expected_departure\"),\n",
    "    col(\"data.actual_departure\"),\n",
    "    col(\"data.capacity\"),\n",
    "    col(\"data.passengers\"),\n",
    "    col(\"data.ticket_price_range\"),\n",
    "    col(\"data.departure_airport\"),\n",
    "    col(\"data.arrival_airport\"),\n",
    "    col(\"data.fuel_consumption\"),\n",
    "    col(\"data.carbon_emission\"),\n",
    "    col(\"data.flight_status\"),\n",
    "    col(\"data.luggage_weight\")\n",
    ")\n",
    "\n",
    "# flat_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1298d0f4-2fcd-43fa-9f17-0f830f016e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d31d010-3616-46ad-afef-cad074a32227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_unixtime,to_timestamp,regexp_replace\n",
    "\n",
    "# Convert STRING scientific notation timestamps to BIGINT\n",
    "flat_df = flat_df.withColumn(\"expected_departure\", col(\"expected_departure\").cast(\"double\").cast(\"bigint\"))\n",
    "flat_df = flat_df.withColumn(\"actual_departure\", col(\"actual_departure\").cast(\"double\").cast(\"bigint\"))\n",
    "\n",
    "# Apply from_unixtime to convert BIGINT to readable timestamps\n",
    "flat_df = flat_df.withColumn(\"expected_departure\", from_unixtime(col(\"expected_departure\")))\n",
    "flat_df = flat_df.withColumn(\"actual_departure\", from_unixtime(col(\"actual_departure\")))\n",
    "from pyspark.sql.functions import col, regexp_replace, to_timestamp\n",
    "\n",
    "# Apply both transformations in a single step\n",
    "flat_df = flat_df.withColumn(\"actual_departure\", to_timestamp(col(\"actual_departure\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                 .withColumn(\"ticket_price_range\", regexp_replace(col(\"ticket_price_range\"), \"\\\\$\", \"\").cast(\"float\")) \\\n",
    "                 .withColumn(\"expected_departure\", to_timestamp(col(\"expected_departure\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1da97a3e-4690-4fdb-b52f-1f800f8e835c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- flight_number: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- route_type: string (nullable = true)\n",
      " |-- expected_departure: timestamp (nullable = true)\n",
      " |-- actual_departure: timestamp (nullable = true)\n",
      " |-- capacity: integer (nullable = true)\n",
      " |-- passengers: integer (nullable = true)\n",
      " |-- ticket_price_range: float (nullable = true)\n",
      " |-- departure_airport: string (nullable = true)\n",
      " |-- arrival_airport: string (nullable = true)\n",
      " |-- fuel_consumption: float (nullable = true)\n",
      " |-- carbon_emission: float (nullable = true)\n",
      " |-- flight_status: string (nullable = true)\n",
      " |-- luggage_weight: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flat_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a17b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "flat_f = flat_df \\\n",
    "    .withColumn(\"delay_minutes\",\n",
    "        (unix_timestamp(\"actual_departure\") - unix_timestamp(\"expected_departure\")) / 60\n",
    "    ) \\\n",
    "    .withColumn(\"occupancy_rate\",\n",
    "        (col(\"passengers\") / col(\"capacity\")) * 100\n",
    "    ) \\\n",
    "    .withColumn(\"flight_duration_minutes\",\n",
    "        (unix_timestamp(\"arrival_time\") - unix_timestamp(\"actual_departure\")) / 60\n",
    "    ) \\\n",
    "    .withColumn(\"is_weekend\",\n",
    "        date_format(\"expected_departure\", \"u\").cast(\"int\") >= 6\n",
    "    ) \\\n",
    "    .withColumn(\"day_part\",\n",
    "        when(hour(\"actual_departure\").between(5, 11), \"Morning\")\n",
    "        .when(hour(\"actual_departure\").between(12, 16), \"Afternoon\")\n",
    "        .when(hour(\"actual_departure\").between(17, 20), \"Evening\")\n",
    "        .otherwise(\"Night\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "726f2b5d-2993-496f-9123-8e66d6257e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm -rf /tmp/spark-checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48585931-4e16-4510-babe-04947386c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_batch(df, batch_id):\n",
    "#     df.show(truncate=False)  # Ensures data prints to terminal only\n",
    "\n",
    "# Modify your streaming query\n",
    "# flat_df.writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .option(\"startingOffsets\", \"latest\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .option(\"checkpointLocation\", \"/tmp/spark-checkpoint\") \\\n",
    "#     .option(\"failOnDataLoss\", \"false\")\\\n",
    "#     .start() \\\n",
    "#     .awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0166d3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1471d3ce-e325-4c17-bd87-732a6d9664c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for query in spark.streams.active:\n",
    "#     query.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7471c5-893f-43cb-97c6-4970dc4ee498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/09 18:13:24 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/06/09 18:13:30 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Batch 12 successfully written to PostgreSQL\n",
      "âœ… Batch 13 successfully written to PostgreSQL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Batch 14 successfully written to PostgreSQL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Batch 15 successfully written to PostgreSQL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Batch 16 successfully written to PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "def write_to_postgres(df, epoch_id):\n",
    "    df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:postgresql://postgres:5432/flight_data\") \\\n",
    "        .option(\"dbtable\", \"flights\") \\\n",
    "        .option(\"user\", \"admin\") \\\n",
    "        .option(\"password\", \"admin\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "    \n",
    "    print(f\"âœ… Batch {epoch_id} successfully written to PostgreSQL\")\n",
    "\n",
    "query = flat_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(write_to_postgres) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/spark-checkpoint\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96765876-35de-4b2b-8f1b-7d38a786dc60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
